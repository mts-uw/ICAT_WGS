{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各種モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.19.2\n",
      "pandas version: 1.1.3\n",
      "scikit-learn version: 0.23.2\n",
      "xgboost version: 1.1.1\n",
      "shap version: 0.36.0\n"
     ]
    }
   ],
   "source": [
    "# coding utf-8\n",
    "## \"coding utf-8\"で日本語のファイルが保存できる。\n",
    "# Data manipulation\n",
    "import numpy as np # numpyモジュールをnpとして取り込め\n",
    "import pandas as pd # pandasモジュールをpdとして取り込め\n",
    "import matplotlib.pyplot as plt # matplotlib.pyplotモジュールをpdとして取り込め\n",
    "%matplotlib inline\n",
    "# Jupyter Notebookで、ノートブック上にグラフを描画する際に指定する記述\n",
    "\n",
    "# Machine learning pipeline\n",
    "import xgboost # XGBoost(勾配ブーストを用いた決定木(GBDT)によるクラス分類や回帰法)モジュールを取り込め\n",
    "import sklearn # scikit-learn (機械学習ライブラリ)モジュールを取り込め\n",
    "import shap    # SHAP (SHapley Additive exPlanations)モジュールを取り込め\n",
    "from sklearn.ensemble import ExtraTreesRegressor # sklearn.ensembleモジュールからExtraTreesRegressorオブジェクトを取り込め\n",
    "from xgboost import XGBRegressor # XGBoostモジュールからXGBRegressorオブジェクトを取り込め\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "# sklearn.model_selectionから KFold, GridSearchCV, train_test_splitオブジェクトを取り込め\n",
    "## KFold(K-分割交差検証): データをk個に分け，n個を訓練用，k-n個をテスト用として使う．分けられたn個のデータがテスト用として必ず1回使われるようにn回検定する．\n",
    "## GridSearchCV(グリッドサーチ): モデルの精度を向上させるために用いられる手法で、全てのパラメータの組み合わせを試してみる方法のこと\n",
    "## train_test_split: 機械学習においてデータを訓練用（学習用）とテスト用に分割してホールドアウト検証を行う\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "# sklearn.metricsモデュールを用いてmeam_squared_error, r2_scoreオブジェクトを取り込め\n",
    "## sklearn.metrics: 作成したモデルの評価を行うモジュール\n",
    "## mean_squared_error: 平均二乗誤差(MSE): 値が小さいほど誤差の少ないモデル\n",
    "\n",
    "\n",
    "import os #OSに依存しているさまざまな機能を利用するためのモジュール\n",
    "# 主にファイルやディレクトリ操作が可能で、ファイルの一覧やpathを取得できたり、新規にファイル・ディレクトリを作成することができる。\n",
    "\n",
    "from skopt.learning import ExtraTreesRegressor as opt_ETR # skopt.learningモジュールからExtraTreesRegressorオブジェクトをopt_ETRとして取り込め\n",
    "from scipy.stats import norm # scipy.statsモジュールからnormオブジェクト(正規分布)を取り込め\n",
    "\n",
    "\n",
    "# Set seed\n",
    "import random # randomモジュールを取り込め\n",
    "random.seed(1107) # 乱数生成器を初期化する。\n",
    "np.random.seed(1107) # np.random.seed(seed=シードに用いる値) をシード(種)を指定することで、発生する乱数をあらかじめ固定する。再現性が必要な場合に用いる。\n",
    "\n",
    "import warnings # 警告の制御を行うモジュール\n",
    "warnings.filterwarnings(\"ignore\") # 実行時の煩わしいwarningを非表示にする。\n",
    "\n",
    "#モジュールのバージョン情報の出力\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "print(\"xgboost version:\", xgboost.__version__)\n",
    "print(\"shap version:\", shap.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各種関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差検証（cross-validation): 汎化性能を評価する統計的な手法\n",
    "def crossvalid(xx, yy, model, cvf):\n",
    "    err_trn = []\n",
    "    err_tes = []\n",
    "    r_2_tes = []\n",
    "    r_2_trn = []\n",
    "    for train_index, test_index in cvf.split(xx):\n",
    "        x_trn = pd.DataFrame(np.array(xx)[train_index], columns =xx.columns)\n",
    "        x_tes = pd.DataFrame(np.array(xx)[test_index], columns = xx.columns)\n",
    "        y_trn = np.array(yy)[train_index]\n",
    "        y_tes = np.array(yy)[test_index]\n",
    "        model.fit(x_trn,y_trn)\n",
    "        x_trn_pred = model.predict(x_trn)\n",
    "        x_tes_pred = model.predict(x_tes)\n",
    "\n",
    "        err_tes.append(mean_squared_error(x_tes_pred, y_tes))\n",
    "        err_trn.append(mean_squared_error(x_trn_pred, y_trn))\n",
    "        r_2_tes.append(r2_score(y_tes, x_tes_pred))\n",
    "        r_2_trn.append(r2_score(y_trn, x_trn_pred))\n",
    "    v_tes = np.sqrt(np.array(err_tes))\n",
    "    v_trn = np.sqrt(np.array(err_trn))\n",
    "    print (\"RMSE %1.3f (sd: %1.3f, min:%1.3f, max:%1.3f, det:%1.3f) ... train\" % (v_trn.mean(), v_trn.std(),v_trn.min(), v_trn.max(),np.array(r_2_trn).mean()))\n",
    "    print (\"RMSE %1.3f (sd: %1.3f, min:%1.3f, max:%1.3f, det:%1.3f) ... test\" % (v_tes.mean(), v_tes.std(), v_tes.min(), v_tes.max(), np.array(r_2_tes).mean()))\n",
    "    # %1.3f: 小数点以下3桁まで表示\n",
    "    ret = {}\n",
    "    ret['trn_mean'] = v_trn.mean()\n",
    "    ret['trn_std'] = v_trn.std()\n",
    "    ret['trn_r2'] = np.array(r_2_trn).mean()\n",
    "    ret['tes_mean'] = v_tes.mean()\n",
    "    ret['tes_std'] = v_tes.std()\n",
    "    ret['tes_r2'] = np.array(r_2_tes).mean()\n",
    "    return ret, v_tes.mean()\n",
    "\n",
    "def plot_importance(model, labels, topk):\n",
    "    \"\"\"\"\n",
    "    Visualize feature importance scores of tree-ensemble model \n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        The tree-ensemble model in scikit-learn.\n",
    "    labels\n",
    "        The labels of the input-values.\n",
    "    topk\n",
    "        The number of feature importance \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize =(6,6))\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "    topk_idx = indices[-topk:]\n",
    "    plt.barh(range(len(topk_idx)), importances[topk_idx], color = 'blue', align = 'center')\n",
    "    plt.yticks(range(len(topk_idx)), labels[topk_idx])\n",
    "    plt.ylim([-1, len(topk_idx)])\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    \n",
    "\n",
    "def one_shot_plot(feat, target, model, xylim = [0,35],  random_state = 1107):\n",
    "    plt.figure()\n",
    "    plt.subplot().set_aspect('equal')\n",
    "    x_train, x_test, y_train, y_test = train_test_split(feat, target, test_size= 0.1, random_state = random_state)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "    plt.plot(y_test, y_test_pred, 'o', c = 'red', markersize = 3, alpha = 0.4, label = 'test')\n",
    "    plt.plot(y_train, y_train_pred, 'o', c = 'blue', markersize = 3, alpha = 0.4,label = 'train')\n",
    "\n",
    "    plt.plot([-100,200],[-100,200], c = '0', ls = '-', lw = 1.0)\n",
    "    plt.xlim(xylim)\n",
    "    plt.ylim(xylim)\n",
    "    plt.xlabel(\"Experimental {} [%]\".format(target.name))\n",
    "    plt.ylabel(\"Predicted {} [%]\".format(target.name))\n",
    "    \n",
    "def smac(model, init_x, init_y, roen_func, desc, random_state = 1107):\n",
    "    \"\"\"\n",
    "    SMAC　のメインプログラム\n",
    "    \"\"\"\n",
    "    model.fit(init_x.loc[:, with_swed2], init_y)\n",
    "    print(model)\n",
    "    mu, sigma =  posterior(init_x.loc[:, with_swed2], init_x.loc[:, with_swed2], init_y, model)\n",
    "    ei =  EI(mu, sigma, init_y.max())\n",
    "    ei = pd.Series(ei, index = init_x.index, name = 'ei')\n",
    "    make_nei =pd.Series(True, index=init_x.index, name='make_nei')\n",
    "    next_x = pd.concat([init_x, ei, make_nei], axis =1)\n",
    "    while next_x['make_nei'].sum() != 0:\n",
    "        next_x = roen_func(next_x, init_y, model, desc)\n",
    "        print(next_x['make_nei'].sum())\n",
    "        \n",
    "    return next_x\n",
    "\n",
    "def posterior(x, p_x, p_y, model):\n",
    "    \"\"\"\n",
    "    EIを計算する上でのμならびにσの計算(鈴木氏作成)\n",
    "    \"\"\"\n",
    "    if len(p_x.shape) == 1:\n",
    "        model.fit(p_x.reshape(-1, 1), p_y)\n",
    "        mu, sigma = model.predict(x.reshape(-1, 1), return_std = True)\n",
    "    else:\n",
    "        model.fit(p_x, p_y)\n",
    "        mu, sigma = model.predict(x, return_std = True)\n",
    "    ind = np.where(sigma == 0)\n",
    "    sigma[ind] = 1e-5\n",
    "    return mu, sigma\n",
    "\n",
    "def EI(mu, sigma, cur_max):\n",
    "    \"\"\"\n",
    "    EIの計算(鈴木氏作成)\n",
    "    \"\"\"\n",
    "    Z = (mu - cur_max)/ sigma\n",
    "    ei = (mu - cur_max) * norm.cdf(Z) + sigma*norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "def opt_function(x, y ,model, desc,random_state =1107):\n",
    "    \"\"\"\n",
    "    各実験ずみの点に対して4回近傍の探索を行い最もEIの高かった組成実験条件を返す\n",
    "    \"\"\"\n",
    "    xx = x.iloc[:,:-2]\n",
    "    neighbor = pd.DataFrame(index = x.columns)\n",
    "    for key, row in xx.iterrows():\n",
    "        count = 0\n",
    "        nei_cand = []\n",
    "        #print('ind={0}'.format(key))\n",
    "        if x.loc[key, 'make_nei'] == True:\n",
    "\n",
    "            for _ in range(30):\n",
    "                row_ch = row.copy()\n",
    "                \n",
    "                row_ch = SWED_change(row_ch, desc)\n",
    "\n",
    "                nei_cand.append(row_ch)\n",
    "                \n",
    "            for _ in range(30):\n",
    "                row_ch = row.copy()\n",
    "                \n",
    "                row_ch = experiment_change(row_ch)\n",
    "                \n",
    "                nei_cand.append(row_ch)\n",
    "                \n",
    "            for _ in range(30):\n",
    "                row_ch = row.copy()\n",
    "                \n",
    "                row_ch = SWED_change(row_ch, desc)\n",
    "                \n",
    "                row_ch = experiment_change(row_ch)\n",
    "                \n",
    "                nei_cand.append(row_ch)\n",
    "                   \n",
    "\n",
    "            nei_cand = pd.DataFrame(nei_cand, index =np.arange(len(nei_cand))).fillna(0)\n",
    "            swed = comp_times_base(nei_cand.loc[:,element],desc.loc[element].T,sort=True,times=True)\n",
    "            swed = pd.DataFrame(swed)\n",
    "            swed = swed.iloc[:, :12].fillna(0)\n",
    "            swed.columns = name2\n",
    "            nei_cand.loc[:, name2] =swed\n",
    "                        \n",
    "            mu, sigma = model.predict(np.array(nei_cand.loc[:, with_swed2]), return_std=True)\n",
    "            ind = y.values.argmax()\n",
    "            cur_max = y.iloc[ind]\n",
    "            ei = EI(mu, sigma, cur_max)\n",
    "            ind = np.argmax(ei)\n",
    "            cand = nei_cand.iloc[ind].copy()\n",
    "            cand['ei'] = ei[ind]\n",
    "            if x.loc[key, 'ei'] < cand['ei']:\n",
    "                cand['make_nei'] = True\n",
    "                neighbor = pd.concat([neighbor, cand], axis = 1)\n",
    "            else:\n",
    "                x.loc[key, 'make_nei'] = False\n",
    "                neighbor = pd.concat([neighbor, x.loc[key,:]], axis = 1)\n",
    "                \n",
    "        else:\n",
    "            neighbor = pd.concat([neighbor, x.loc[key,:]], axis = 1)\n",
    "                                                    \n",
    "    print('-----------')\n",
    "    \n",
    "    neighbor = neighbor.T\n",
    "    neighbor.index = x.index\n",
    "    return neighbor\n",
    "\n",
    "\n",
    "\n",
    "def SWED_change(row_ch, desc):\n",
    "    row_ch[element] = 0\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        #i-1回目の処理のデータを保存\n",
    "        row_sub = row_ch\n",
    "        x_ch =data\n",
    "\n",
    "        #大きい方からchange_f番目の元素に紐ずくaddtional descriptorを一括での局所的探索\n",
    "        change_col= list([f\"{i+1}_electronegativity\", f\"{i+1}_delta_fus H\",f\"{i+1}_density\"])\n",
    "        a = row_ch[change_col] - x_ch[change_col].min()\n",
    "        b = x_ch[change_col].max() - x_ch[change_col].min()\n",
    "        v = a/b\n",
    "\n",
    "        v = np.minimum(v, 1.0)\n",
    "\n",
    "        p = np.array([-1])\n",
    "        count = 0\n",
    "        while (p <0).any() | (p>1).any():\n",
    "            p = random.normalvariate(v, 0.1)\n",
    "            count += 1\n",
    "            if (count % 1000) == 0:\n",
    "                print(\"count:\", count)\n",
    "\n",
    "        p = p * b + x_ch[change_col].min()\n",
    "        row_ch[change_col] = p\n",
    "        p = np.array(p)\n",
    "        \n",
    "        #alpha = x1_x0/x1_2#２次関数の軸の位置\n",
    "        alpha = desc.apply(lambda u: np.dot(u, p)/np.dot(u, u), axis = 1)\n",
    "        #εの値が小さい元素名を返す\n",
    "        epsilon = desc.mul(alpha, axis=0).apply(lambda u: np.dot(u-p, u-p), axis=1).sort_values().index\n",
    "\n",
    "\n",
    "        for i in range(0, len(epsilon)):\n",
    "            #軸が100以下をみたし、　組成元素が0ではない元素に変換する\n",
    "            if (alpha[epsilon[i]] < 100) & (row_ch[epsilon[i]] == 0):\n",
    "                row_ch[epsilon[i]] = alpha[epsilon[i]]\n",
    "                break\n",
    "\n",
    "        #組成の合計が100を超えると処理を抜ける        \n",
    "        if row_ch[element].sum() >= 100:\n",
    "            break\n",
    "\n",
    "    # 100によりちかい組成を選択肢、標準化\n",
    "    if  abs(100 -row_ch[element].sum()) > abs(100-row_sub[element].sum()):\n",
    "        row_ch[element] =  (row_sub[element] * 100) / row_sub[element].sum()\n",
    "\n",
    "    else:\n",
    "        row_ch[element] =  (row_ch[element] * 100) / row_ch[element].sum()\n",
    "    \n",
    "    return row_ch\n",
    "\n",
    "def experiment_change(row_ch):\n",
    "    cols =list(preparation) + list(experiment)\n",
    "    x_ch = data\n",
    "    \n",
    "    change_f = random.choice(cols)\n",
    "    \n",
    "    if change_f in experiment:\n",
    "        a = np.array(row_ch[change_f]) - x_ch[change_f].min()\n",
    "        b = x_ch[change_f].max() - x_ch[change_f].min()\n",
    "        v = a / b\n",
    "        p = -1\n",
    "        while (p < 0) | (p > 1):\n",
    "            p = random.normalvariate(v, 0.1)\n",
    "\n",
    "        p = p * b + x_ch[change_f].min()\n",
    "        row_ch[change_f] = p\n",
    "\n",
    "    elif change_f in preparation:\n",
    "        row_ch[change_f] = 1\n",
    "        ind = set(preparation) - {change_f}\n",
    "        row_ch[ind] = 0\n",
    "        \n",
    "    return row_ch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total # of Data</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Data</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Ru</th>\n",
       "      <th>Rh</th>\n",
       "      <th>Ir</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Pd</th>\n",
       "      <th>...</th>\n",
       "      <th>3_density</th>\n",
       "      <th>3_ionization enegy</th>\n",
       "      <th>4_AW</th>\n",
       "      <th>4_atomic radius</th>\n",
       "      <th>4_electronegativity</th>\n",
       "      <th>4_m. p.</th>\n",
       "      <th>4_b. p.</th>\n",
       "      <th>4_delta_fus H</th>\n",
       "      <th>4_density</th>\n",
       "      <th>4_ionization enegy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Andreeva et al.[S1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.879971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.646330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.421297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.879971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.646330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total # of Data            Reference  Data   Pt        Au   Ru   Rh   Ir  \\\n",
       "0                1  Andreeva et al.[S1]     1  0.0  0.879971  0.0  0.0  0.0   \n",
       "1                2                  NaN     2  0.0  2.646330  0.0  0.0  0.0   \n",
       "2                3                  NaN     3  0.0  4.421297  0.0  0.0  0.0   \n",
       "3                4                  NaN     4  0.0  0.879971  0.0  0.0  0.0   \n",
       "4                5                  NaN     5  0.0  2.646330  0.0  0.0  0.0   \n",
       "\n",
       "    Cu   Pd  ...  3_density  3_ionization enegy  4_AW  4_atomic radius  \\\n",
       "0  0.0  0.0  ...        0.0                 0.0   0.0              0.0   \n",
       "1  0.0  0.0  ...        0.0                 0.0   0.0              0.0   \n",
       "2  0.0  0.0  ...        0.0                 0.0   0.0              0.0   \n",
       "3  0.0  0.0  ...        0.0                 0.0   0.0              0.0   \n",
       "4  0.0  0.0  ...        0.0                 0.0   0.0              0.0   \n",
       "\n",
       "   4_electronegativity  4_m. p.  4_b. p.  4_delta_fus H  4_density  \\\n",
       "0                  0.0      0.0      0.0            0.0        0.0   \n",
       "1                  0.0      0.0      0.0            0.0        0.0   \n",
       "2                  0.0      0.0      0.0            0.0        0.0   \n",
       "3                  0.0      0.0      0.0            0.0        0.0   \n",
       "4                  0.0      0.0      0.0            0.0        0.0   \n",
       "\n",
       "   4_ionization enegy  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"data/WGS.xlsx\", skiprows = 8)\n",
    "drop_cols = [\"ZEO\", \"HAP\", \"ACC\", \"YSZ\"]\n",
    "idx = (data.loc[:, drop_cols] == 0).all(axis =1)\n",
    "data = data[idx].drop(drop_cols, axis = 1)\n",
    "data.index = np.arange(len(data))\n",
    "\n",
    "base_metal = data.loc[:,\"Pt\":\"Pd\"].columns\n",
    "preparation = data.loc[:,\"IWI\":\"DP\"].columns\n",
    "cal_cond = data.loc[:,\"Calcination Temperture (℃)\":\"Calc T. (hr)\"].columns\n",
    "support = data.loc[:,\"Al2O3\":\"CaO\"].columns\n",
    "promoter= data.loc[:,\"Li\":\"Sr\"].columns\n",
    "exp_cond = data.loc[:,\"Reaction Temperture (℃)\":\"F/W (mg.min/ml)\"].columns\n",
    "element =  list(base_metal)  +list(support) + list(promoter)\n",
    "experiment =  list(cal_cond) + list(exp_cond)\n",
    "\n",
    "desc = pd.read_csv(\"data/Descriptors_WGS.csv\", skiprows=[0], index_col=\"symbol\")\n",
    "desc.drop(\n",
    "    [\n",
    "        \"Unnamed: 0\",\n",
    "        \"AN\",\n",
    "        \"name\",\n",
    "        \"period\",\n",
    "        \"ionic radius\",\n",
    "        \"covalent radius\",\n",
    "        \"group\",\n",
    "        \"VdW radius\",\n",
    "        \"crystal radius\",\n",
    "        \"a x 106 \",\n",
    "        \"Heat capacity \",\n",
    "        \"l\",\n",
    "        \"electron affinity \",\n",
    "        \"VE\",\n",
    "        \"Surface energy \",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "desc = desc.fillna(desc.mean())\n",
    "\n",
    "sub =100 - data.loc[:, base_metal].sum(axis = 1) - data.loc[:, promoter].sum(axis = 1)\n",
    "data.loc[:, support] = pd.DataFrame(np.array(data.loc[:, support]) *np.array(sub).reshape(-1, 1), columns = support)\n",
    "support_data = pd.read_excel(\"data/support.xlsx\", index_col = 0)\n",
    "\n",
    "data.loc[:, support] = data.loc[:,  support]/  support_data.loc[:, \"ave_MW\"].T\n",
    "data.loc[:, base_metal] = data.loc[:, list(base_metal)]/desc.loc[list(base_metal) , \"AW\"].T\n",
    "data.loc[:, promoter] = data.loc[:, list(promoter)]/desc.loc[list(promoter),  \"AW\"].T\n",
    "data.loc[:, element]  =  pd.DataFrame(np.array(data.loc[:, element])/np.array(data.loc[:, element].sum(axis = 1)).reshape(-1,1),\n",
    "                                      columns = element)*100\n",
    "\n",
    "for i in support_data.index:\n",
    "    key =  support_data.loc[i, \"key\"]\n",
    "    if key in promoter:\n",
    "        data.loc[:, key] = data.loc[:, key] + data.loc[:, i]\n",
    "        \n",
    "    else:\n",
    "        data.loc[:, key] =  data.loc[:, i]\n",
    "        \n",
    "element =list(base_metal)+ list(promoter)+ [\"Al\", \"Tb\", \"Hf\", \"Th\", \"Si\"] \n",
    "conventional =  list(base_metal) + list(preparation) + list(cal_cond) + list(promoter)+ [\"Al\", \"Tb\", \"Hf\", \"Th\", \"Si\"] + list(exp_cond)\n",
    "\n",
    "def comp_times_base(comp, base, sort=False, times=True, attention=False):\n",
    "    count = 0\n",
    "    for key, rows in comp.iterrows():\n",
    "        stack = np.vstack((rows, base))\n",
    "        if times == True:\n",
    "            time = np.array(base) * np.array(rows)\n",
    "            stack = np.vstack((rows, time))\n",
    "\n",
    "        if sort == True:\n",
    "            stack = pd.DataFrame(stack).sort_values(\n",
    "                [0], ascending=False, axis=1)\n",
    "\n",
    "        stack = pd.DataFrame(stack).iloc[1:, :]\n",
    "        stack = np.array(stack)\n",
    "\n",
    "        if count == 0:\n",
    "            if attention:\n",
    "                res = np.sum(stack, axis=1)\n",
    "            else:\n",
    "                res = np.array(stack.T.flatten())\n",
    "\n",
    "            count += 1\n",
    "        else:\n",
    "            if attention:\n",
    "                res = np.vstack((res, np.sum(stack, axis=1)))\n",
    "            else:\n",
    "                res = np.vstack((res, np.array(stack.T.flatten())))\n",
    "\n",
    "            count += 1\n",
    "    return res\n",
    "name = []\n",
    "for i in range(1, 5):\n",
    "    #name.append('%i_AN'%i)\n",
    "    name.append('%i_AW'%i)\n",
    "    #name.append('%i_group'%i)\n",
    "    #name.append('%i_period'%i)\n",
    "    name.append('%i_atomic radius'%i)\n",
    "    name.append('%i_electronegativity'%i)\n",
    "    name.append('%i_m. p.'%i)\n",
    "    name.append('%i_b. p.'%i)\n",
    "    name.append('%i_delta_fus H'%i)\n",
    "    name.append('%i_density'%i)\n",
    "    name.append('%i_ionization enegy'%i)\n",
    "    \n",
    "name2 = []\n",
    "for i in range(1, 5):\n",
    "    #name2.append('%i_AN'%i)\n",
    "    #name2.append('%i_AW'%i)\n",
    "    #name2.append('%i_group'%i)\n",
    "    #name2.append('%i_period'%i)\n",
    "    #name2.append('%i_atomic radius'%i)\n",
    "    name2.append('%i_electronegativity'%i)\n",
    "    #name2.append('%i_m. p.'%i)\n",
    "    #name2.append('%i_b. p.'%i)\n",
    "    name2.append('%i_delta_fus H'%i)\n",
    "    name2.append('%i_density'%i)\n",
    "    #name2.append('%i_ionization enegy'%i)\n",
    "    \n",
    "\n",
    "swed =pd.DataFrame(comp_times_base(data.loc[:, element], desc.loc[element].T, sort = True)).iloc[:, :32]\n",
    "swed.columns = name\n",
    "data = pd.concat([data, swed], axis= 1)\n",
    "\n",
    "with_swed = name  + list(preparation) + list(cal_cond) + list(exp_cond)\n",
    "with_swed2 = name2 + list(preparation) + list(cal_cond) + list(exp_cond)\n",
    "smac_swed = element +name2 + list(preparation) + list(cal_cond) + list(exp_cond)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concentional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2c2d6d97969d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"conventional\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'CO Conversion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"out/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'columns' is not defined"
     ]
    }
   ],
   "source": [
    "folder = \"conventional\"\n",
    "feat = data.loc[:, columns]\n",
    "target =data.loc[:,  'CO Conversion']\n",
    "path = \"out/\" + folder + \"/\"\n",
    "\n",
    "os.makedirs(path, exist_ok = True)\n",
    "cvf = KFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "print(\"ETR\")\n",
    "cvmodel = GridSearchCV(ExtraTreesRegressor(n_jobs = -1, random_state = 1107),\n",
    "                      param_grid = {\"n_estimators\": [250, 500, 1000]}, n_jobs = -1)\n",
    "crossvalid(feat, target, cvmodel, cvf)\n",
    "\n",
    "cvmodel.fit(feat, target)\n",
    "model = cvmodel.best_estimator_\n",
    "\n",
    "plot_importance(model, feat.columns, 20)\n",
    "plt.savefig(path + \"importance_ETR.png\", bbox_inches = \"tight\", dpi = 300)\n",
    "\n",
    "one_shot_plot(feat, target, model, xylim = [0, 100],  random_state =1107)\n",
    "plt.savefig(path + \"one_shot_ETR.png\", bbox_inches = \"tight\", dpi = 300)\n",
    "\n",
    "print(\"XGB\")\n",
    "cvmodel = GridSearchCV(XGBRegressor(n_jobs = -1, random_state = 1107),\n",
    "                      param_grid = {\"n_estimators\": [250, 500, 1000]}, n_jobs = -1)\n",
    "crossvalid(feat, target, cvmodel, cvf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with SWED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2f2c8b099581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m cvmodel = GridSearchCV(ExtraTreesRegressor(n_jobs = -1, random_state = 1126),\n\u001b[1;32m     11\u001b[0m                       param_grid = {\"n_estimators\": [250, 500, 1000]}, n_jobs = -1)\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcrossvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcvmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d8d7874a65b3>\u001b[0m in \u001b[0;36mcrossvalid\u001b[0;34m(xx, yy, model, cvf)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0my_trn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_tes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx_trn_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx_tes_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folder = \"with_swed\"\n",
    "feat = data.loc[:, with_swed]\n",
    "target =data.loc[:,  'CO Conversion']\n",
    "path = \"out/\" + folder + \"/\"\n",
    "\n",
    "os.makedirs(path, exist_ok = True)\n",
    "cvf = KFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "print(\"ETR\")\n",
    "cvmodel = GridSearchCV(ExtraTreesRegressor(n_jobs = -1, random_state = 1107),\n",
    "                      param_grid = {\"n_estimators\": [250, 500, 1000]}, n_jobs = -1)\n",
    "crossvalid(feat, target, cvmodel, cvf)\n",
    "\n",
    "cvmodel.fit(feat, target)\n",
    "model = cvmodel.best_estimator_\n",
    "\n",
    "plot_importance(model, feat.columns, 20)\n",
    "plt.savefig(path + \"importance_ETR.png\", bbox_inches = \"tight\", dpi = 300)\n",
    "\n",
    "one_shot_plot(feat, target, model, xylim = [0, 100],  random_state = 1107)\n",
    "plt.savefig(path + \"one_shot_ETR.png\", bbox_inches = \"tight\", dpi = 300)\n",
    "\n",
    "print(\"XGB\")\n",
    "cvmodel = GridSearchCV(XGBRegressor(n_jobs = -1, random_state = 1107),\n",
    "                      param_grid = {\"n_estimators\": [250, 500, 1000]}, n_jobs = -1)\n",
    "crossvalid(feat, target, cvmodel, cvf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with SWED (3 descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"with_swed2\"\n",
    "feat = data.loc[:, with_swed2]\n",
    "target =data.loc[:,  'CO Conversion']\n",
    "path = \"out/\" + folder + \"/\"\n",
    "\n",
    "os.makedirs(path, exist_ok = True)\n",
    "cvf = KFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "print(\"ETR\")\n",
    "cvmodel = GridSearchCV(ExtraTreesRegressor(n_jobs = -1, random_state = 1107),\n",
    "                      param_grid = {\"n_estimators\": [250, 500, 1000]}, n_jobs = -1)\n",
    "crossvalid(feat, target, cvmodel, cvf)\n",
    "\n",
    "cvmodel.fit(feat, target)\n",
    "model = cvmodel.best_estimator_\n",
    "\n",
    "plot_importance(model, feat.columns, 20)\n",
    "plt.savefig(path + \"importance_ETR.png\", bbox_inches = \"tight\", dpi = 300)\n",
    "\n",
    "one_shot_plot(feat, target, model, xylim = [0, 100],  random_state =1107)\n",
    "plt.savefig(path + \"one_shot_ETR.png\", bbox_inches = \"tight\", dpi = 300)\n",
    "\n",
    "print(\"XGB\")\n",
    "cvmodel = GridSearchCV(XGBRegressor(n_jobs = -1, random_state = 1107),\n",
    "                      param_grid = {\"n_estimators\": [250, 500, 1000]}, n_jobs = -1)\n",
    "crossvalid(feat, target, cvmodel, cvf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"conventional\"\n",
    "feat = data.loc[:, conventional]\n",
    "target =data.loc[:,  'CO Conversion']\n",
    "path = \"out/\" + folder + \"/\"\n",
    "\n",
    "cvmodel = GridSearchCV(XGBRegressor(n_jobs = -1, random_state = 1107),\n",
    "                      param_grid = {\"n_estimators\": [250, 500, 1000]}, n_jobs = -1)\n",
    "\n",
    "cvmodel.fit(feat, target)\n",
    "\n",
    "model = cvmodel.best_estimator_\n",
    "explainer = shap.TreeExplainer(model=model)\n",
    "shap_values = explainer.shap_values(feat) \n",
    "\n",
    "shap.summary_plot(shap_values, feat, show = False)\n",
    "plt.savefig(\"out/conventional/shap.png\", format = \"png\", dpi = 600, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "#shap.force_plot(base_value=explainer.expected_value, shap_values=shap_values, features=feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"smac\"\n",
    "idx = data.loc[:, 'Reaction Temperture (℃)'] <= 150\n",
    "data = data[idx]\n",
    "feat = data.loc[:, smac_swed]\n",
    "target =data.loc[:,  'CO Conversion']\n",
    "path = \"out/\" + folder + \"/\"\n",
    "os.makedirs(path, exist_ok = True)\n",
    "\n",
    "print(feat.shape)\n",
    "\n",
    "cvmodel = GridSearchCV(ExtraTreesRegressor(n_jobs = -1, random_state = 1107),\n",
    "                      param_grid = {\"n_estimators\": [250, 500, 1000]}, n_jobs = -1)\n",
    "cvmodel.fit(feat.loc[:, smac_swed], target)\n",
    "n_tree = cvmodel.best_params_[\"n_estimators\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    " model = opt_ETR(n_estimators = n_tree, random_state = 1107, n_jobs = -1)\n",
    "cand = smac(model, feat, target,opt_function, desc.loc[element, ['electronegativity', 'delta_fus H ', 'density']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "feat = feat.loc[:, with_swed2]\n",
    "model = opt_ETR(n_estimators = 500, n_jobs = -1)\n",
    "k = KMeans(n_clusters=40, random_state = 1107)\n",
    "cluster = k.fit_predict(cand.iloc[:,:-1])\n",
    "cluster = pd.Series(cluster, index=cand.index, name='cluster')\n",
    "cand = pd.concat([cand,cluster], axis=1)\n",
    "model.fit(feat, target)\n",
    "pred_y = model.predict(cand.loc[:,with_swed2])\n",
    "pred_y = pd.Series(pred_y, index = cand.index, name = 'pred_y')\n",
    "cand = pd.concat([cand,pred_y], axis = 1)\n",
    "#choose point which has most better ei value in each cluster\n",
    "clus_high = cand.sort_values(by=['cluster','ei']).drop_duplicates(subset=['cluster'],keep='last')\n",
    "clus_high = clus_high.sort_values(by='ei', ascending=False)\n",
    "\n",
    "hogege = []\n",
    "for key,row in clus_high.loc [:,element].iterrows():\n",
    "    temp = [str(i)+':'+str(round(v,1)) for i,v in row[row>0].sort_values(ascending=False).iteritems()]\n",
    "    hogege.append(temp)\n",
    "    \n",
    "hogege = [' '.join(x) for x in hogege]\n",
    "\n",
    "w = 0.4\n",
    "hoge = clus_high.iloc[:20]\n",
    "x = np.arange(hoge.shape[0])\n",
    "pred_y = list(clus_high['pred_y'])\n",
    "    \n",
    "                \n",
    "extra = []\n",
    "for y in x:\n",
    "    extra.append(y)\n",
    "    \n",
    "ytick = []\n",
    "for n in range(20):\n",
    "    ytick.append(hogege[n])\n",
    "    \n",
    "plt.figure(figsize=(6,6), dpi=600)\n",
    "plt.barh(x,hoge['ei'][::-1],label='EI', color = 'blue')\n",
    "for n,i in enumerate(x[::-1]):\n",
    "    plt.text(clus_high['ei'].iloc[n],i-0.4,str(round(clus_high['ei'].iloc[n],2)),fontsize=12)\n",
    "\n",
    "\n",
    "plt.xlim([0,10])\n",
    "plt.yticks(x[::-1],ytick)\n",
    "#temp =clus_high.loc[:,\"Temperature, K\":\"ei\"]\n",
    "#temp.index = hogege\n",
    "#temp.to_csv(\"Table_3.csv\")\n",
    "plt.savefig('WGS_below150.png', dpi = 600, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand = cand.iloc[:, :-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
